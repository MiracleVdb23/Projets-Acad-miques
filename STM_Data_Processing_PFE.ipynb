{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the large volume of data processing involved, this part of the workflow was implemented using Google Colab, a cloud-based platform for executing Python code interactively. Google Colab provides access to powerful computing resources, including GPUs and TPUs, which are well-suited for handling large datasets and computationally intensive tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z74gYpf-1KAH"
   },
   "source": [
    "### Library Installation and Google Drive Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC03xrKkKwee"
   },
   "outputs": [],
   "source": [
    "# Installing PyDrive and importing necessary modules\n",
    "!pip install -U -q PyDrive\n",
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDBJYjl21WLw"
   },
   "source": [
    "### Interacting with Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJrK31riKtKo",
    "outputId": "bab2d5ce-50b0-4750-edd6-3011d0546f44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
     ]
    }
   ],
   "source": [
    "# Interagir avec Google Drive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Traitement des données\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Traitement distribué des données\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, avg, first, expr, stddev\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialisez un DataFrame Spark vide\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "# Visualisation des données\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Manipulation des données Spark\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Accéder aux fichiers Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "# Imprimer les données\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn9empB216M5"
   },
   "source": [
    "### Downloading Files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELJtYCZ7cZIx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Authentication and creation of PyDrive client\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Choosing the directory to contain downloaded files\n",
    "folder = os.path.expanduser('~/my-directory')\n",
    "try:\n",
    "  os.makedirs(folder)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Browsing through the Google Drive directory and downloading files\n",
    "file_list = drive.ListFile(\n",
    "    {'q': \"'1IBNfVfLCinWTFpSgku8x9--evi1q-o6y' in parents\"}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "  name = os.path.join(folder, file['title'])\n",
    "  print('Downloading {}'.format(name))\n",
    "  new_file = drive.CreateFile({'id': file['id']})\n",
    "  new_file.GetContentFile(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdJutKI_tOeJ"
   },
   "outputs": [],
   "source": [
    "!ls -lha /root/my-directory/prepared0107.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEOMyWGS2EJ3"
   },
   "source": [
    "### Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR6nMhp3MYEf"
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .config(\"spark.executor.memory\", \"15g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5g9tag81h7q"
   },
   "source": [
    "### Reading CSV Files and Creating a Input Spark DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHh3KiMOd_Cq"
   },
   "outputs": [],
   "source": [
    "# Specify the local directory\n",
    "dossier = '/root/my-directory'  # Modify the path accordingly\n",
    "\n",
    "# Get the list of all CSV files in the directory\n",
    "fichiers_csv = glob(os.path.join(dossier, '*.csv'))\n",
    "\n",
    "# Define your input schema\n",
    "schema_input = StructType([\n",
    "    StructField(\"TIMESTAMP\", TimestampType(), True),\n",
    "    StructField(\"BATCH_ID\", LongType(), True),\n",
    "    StructField(\"INPUT_MAPPING_Y\", IntegerType(), True),\n",
    "    StructField(\"INPUT_MAPPING_X\", IntegerType(), True),\n",
    "    StructField(\"ZONE\", IntegerType(), True),\n",
    "    StructField(\"PHASE\", StringType(), True),\n",
    "    StructField(\"PARAM_1\", DoubleType(), True),\n",
    "    StructField(\"PARAM_2\", DoubleType(), True),\n",
    "    StructField(\"PARAM_3\", DoubleType(), True),\n",
    "    StructField(\"PARAM_4\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Initialize an empty global DataFrame\n",
    "dataframe_global = None\n",
    "\n",
    "# Iterate through the CSV files in the local directory\n",
    "for fichier_chemin in fichiers_csv:\n",
    "    # Read the CSV file into a Spark DataFrame with the explicit schema\n",
    "    dataframe_fichier = spark.read.csv(fichier_chemin, header=True, schema=schema_input)\n",
    "\n",
    "    # If the global DataFrame hasn't been defined yet, use the schema from the first file\n",
    "    if dataframe_global is None:\n",
    "        dataframe_global = spark.createDataFrame([], schema=schema_input)\n",
    "\n",
    "    # Ensure columns match before adding to the global DataFrame\n",
    "    dataframe_fichier = dataframe_fichier.select(*schema_input.fieldNames())\n",
    "\n",
    "    # Union the DataFrames\n",
    "    dataframe_global = dataframe_global.union(dataframe_fichier)\n",
    "\n",
    "# Show the global DataFrame\n",
    "dataframe_global.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvuScelFxWdF"
   },
   "outputs": [],
   "source": [
    "\n",
    "nombre_de_lignes_in = dataframe_global.count()\n",
    "nombre_de_colonnes_in = len(dataframe_global.columns)\n",
    "\n",
    "print(\"Nombre de lignes input filtré:\", nombre_de_lignes_in)\n",
    "print(\"Nombre de colonnes input filtré:\", nombre_de_colonnes_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG5zYpc62psi"
   },
   "source": [
    "### Authentication and Downloading Files from Google Drive (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZFICo5nh4qF",
    "outputId": "a3718ed2-6d0b-4d8c-d4f1-aec8cc55005c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/mon-repertoire_output/df_results.xlsx\n",
      "Downloading /root/mon-repertoire_output/data_final_A.xlsx\n",
      "Downloading /root/mon-repertoire_output/lot_strip_relation_joined_windows_prepared.csv\n"
     ]
    }
   ],
   "source": [
    "# Authentication and creating PyDrive client\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Choosing the directory to store downloaded files\n",
    "directory = os.path.expanduser('~/mon-repertoire_output')\n",
    "try:\n",
    "  os.makedirs(directory)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# Traversing the Google Drive directory and downloading files\n",
    "file_list = drive.ListFile(\n",
    "    {'q': \"'1otDaK3HIoawL_wVb55OBBRNUuAGZ-JUh' in parents\"}).GetList()\n",
    "\n",
    "for file in file_list:\n",
    "  name = os.path.join(directory, file['title'])\n",
    "  print('Downloading {}'.format(name))\n",
    "  new_file = drive.CreateFile({'id': file['id']})\n",
    "  new_file.GetContentFile(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by8ficEf2yi4"
   },
   "source": [
    "### Spark Initialization and Reading the Downloaded CSV File (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YbACwaFabj5"
   },
   "outputs": [],
   "source": [
    "# Create the full path of the file\n",
    "file_path = '/root/mon-repertoire_output/lot_strip_relation_joined_windows_prepared.csv'\n",
    "\n",
    "# Initialize an empty Spark DataFrame\n",
    "schema = None\n",
    "dataframe_global_output = None\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "dataframe_global_output = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "schema = dataframe_global_output.schema\n",
    "\n",
    "# Show the global DataFrame\n",
    "dataframe_global_output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f-APziCxJxn"
   },
   "outputs": [],
   "source": [
    "nombre_de_lignes_out = dataframe_global_output.count()\n",
    "nombre_de_colonnes_out = len(dataframe_global_output.columns)\n",
    "\n",
    "print(\"Number of output rows:\", nombre_de_lignes_out)\n",
    "print(\"Number of output columns:\", nombre_de_colonnes_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySluDOZi241c"
   },
   "source": [
    "### Filtering the Input DataFrame Based on BATCH_IDs Present in the Output DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yl5p1yjDdxj9"
   },
   "outputs": [],
   "source": [
    "# Filter the input database based on the BATCH_IDs present in the output database\n",
    "dataframe_filtered_input = dataframe_global.filter(col('BATCH_ID').isin(dataframe_global_output.select('BATCH_ID').distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "\n",
    "# Show the filtered DataFrame of the input database\n",
    "dataframe_filtered_input.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfm83iZ-2_9r"
   },
   "source": [
    "### Calculating Statistics for Numeric Columns Grouped by \"PHASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7GEdKdngCz_"
   },
   "outputs": [],
   "source": [
    "# Select the numeric columns for which you want to calculate statistics\n",
    "numeric_columns = ['PARAM_1', 'PARAM_2', 'PARAM_3', 'PARAM_4']\n",
    "\n",
    "# Group by \"PHASE\" and calculate the mean for each numeric column\n",
    "dataframe_filtered_input.groupBy(\"PHASE\") \\\n",
    "    .agg(mean(\"PARAM_1\").alias(\"avg_p1\"), \\\n",
    "         mean(\"PARAM_2\").alias(\"avg_p2\"), \\\n",
    "         mean(\"PARAM_3\").alias(\"avg_p3\"), \\\n",
    "         mean(\"PARAM_4\").alias(\"avg_p4\"), \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTblfg4dldR2"
   },
   "source": [
    "### Displaying Data for Phase A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTuvDAGxe4UK"
   },
   "outputs": [],
   "source": [
    "# Filter the input database based on the BATCH_IDs present in the output database\n",
    "data_input_A = dataframe_filtered_input.filter(col(\"PHASE\") == \"PHASE_A\")\n",
    "\n",
    "# Show the filtered input DataFrame for PHASE_A\n",
    "data_input_A.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3upwkI1J3QGQ"
   },
   "source": [
    "### Correlation Between Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oUmyNez9wm-"
   },
   "outputs": [],
   "source": [
    "# Select columns A, B, C, and D\n",
    "selected_columns = ['PARAM_1', 'PARAM_2', 'PARAM_3', 'PARAM_4']\n",
    "selected_data = data_input_A.select(selected_columns)\n",
    "selected_data = selected_data.na.drop()\n",
    "\n",
    "# Assemble the columns into a vector\n",
    "assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(selected_data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = Correlation.corr(assembled_data, \"features\").head()\n",
    "\n",
    "# Obtenez la matrice DenseMatrix\n",
    "dense_matrix = correlation_matrix[0]\n",
    "\n",
    "# Convertir la matrice dense en une liste de listes\n",
    "correlation_matrix_list = dense_matrix.toArray().tolist()\n",
    "\n",
    "# Créer un DataFrame pandas à partir de la liste de listes\n",
    "columns = selected_columns\n",
    "corr_df = pd.DataFrame(correlation_matrix_list, index = columns, columns=columns)\n",
    "\n",
    "# Tracer la heatmap avec les noms de colonnes\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMeeqblfVpk8"
   },
   "outputs": [],
   "source": [
    "# Sort the DataFrame based on the timestamp column in ascending order\n",
    "data_input_A = data_input_A.orderBy(\"TIMESTAMP\")\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "data_input_A.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mUakNxm3fZe"
   },
   "source": [
    "### Grouping by BATCH_ID, PHASE, INPUT_MAPPING_X, and INPUT_MAPPING_Y with Statistics Calculation for Each Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z87P78OOBCd6"
   },
   "outputs": [],
   "source": [
    "# Group by BATCH_ID, PHASE, INPUT_MAPPING_X, and INPUT_MAPPING_Y, calculate statistics for each parameter\n",
    "data_input_A = data_input_A.groupBy(\"BATCH_ID\", \"PHASE\", \"INPUT_MAPPING_X\", \"INPUT_MAPPING_Y\") \\\n",
    "    .agg(\n",
    "        first(\"TIMESTAMP\").alias(\"date_input\"),\n",
    "        avg(\"PARAM_1\").alias(\"avg_param_1\"),\n",
    "        expr(\"percentile(PARAM_1, 0.25)\").alias(\"q1_param_1\"),\n",
    "        expr(\"percentile(PARAM_1, 0.5)\").alias(\"q2_param_1\"),\n",
    "        expr(\"percentile(PARAM_1, 0.75)\").alias(\"q3_param_1\"),\n",
    "        stddev(\"PARAM_1\").alias(\"stddev_param_1\"),\n",
    "        avg(\"PARAM_2\").alias(\"avg_param_2\"),\n",
    "        expr(\"percentile(PARAM_2, 0.25)\").alias(\"q1_param_2\"),\n",
    "        expr(\"percentile(PARAM_2, 0.5)\").alias(\"q2_param_2\"),\n",
    "        expr(\"percentile(PARAM_2, 0.75)\").alias(\"q3_param_2\"),\n",
    "        stddev(\"PARAM_2\").alias(\"stddev_param_2\"),\n",
    "        avg(\"PARAM_3\").alias(\"avg_param_3\"),\n",
    "        expr(\"percentile(PARAM_3, 0.25)\").alias(\"q1_param_3\"),\n",
    "        expr(\"percentile(PARAM_3, 0.5)\").alias(\"q2_param_3\"),\n",
    "        expr(\"percentile(PARAM_3, 0.75)\").alias(\"q3_param_3\"),\n",
    "        stddev(\"PARAM_3\").alias(\"stddev_param_3\"),\n",
    "        avg(\"PARAM_4\").alias(\"avg_param_4\"),\n",
    "        expr(\"percentile(PARAM_4, 0.25)\").alias(\"q1_param_4\"),\n",
    "        expr(\"percentile(PARAM_4, 0.5)\").alias(\"q2_param_4\"),\n",
    "        expr(\"percentile(PARAM_4, 0.75)\").alias(\"q3_param_4\"),\n",
    "        stddev(\"PARAM_4\").alias(\"stddev_param_4\")\n",
    "    )\n",
    "\n",
    "# Display the result\n",
    "data_input_A.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUT9zUEEDIVY",
    "outputId": "583b1ed9-8c70-4126-8508-a545b6d62b17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct batch IDs: 55555\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame based on the timestamp column in ascending order\n",
    "data_input_A = data_input_A.orderBy(\"BATCH_ID\", \"PHASE\", \"INPUT_MAPPING_X\", \"INPUT_MAPPING_Y\", \"date_input\")\n",
    "\n",
    "number_of_distinct_batch_ids = data_input_A.count()\n",
    "\n",
    "print(\"Number of distinct batch IDs:\", number_of_distinct_batch_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQZa36653qD7"
   },
   "source": [
    "### Adding OUTPUT_MAPPING_X and OUTPUT_MAPPING_Y Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdw7jkVHMpJv"
   },
   "outputs": [],
   "source": [
    "# Add the columns iox and ioy\n",
    "data_input_A = data_input_A.withColumn(\"OUTPUT_MAPPING_X\", 16 - col(\"INPUT_MAPPING_X\")).withColumn(\"OUTPUT_MAPPING_Y\", 4 - col(\"INPUT_MAPPING_Y\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "data_input_A.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYam2rH8MkS1"
   },
   "outputs": [],
   "source": [
    "# Group by batch_id, phase, input_x, and input_y, calculate the mean for each parameter\n",
    "dataframe_global_output = dataframe_global_output.orderBy(\"BATCH_ID\", \"OUTPUT_MAPPING_X\", \"OUTPUT_MAPPING_Y\")\n",
    "\n",
    "# Show the result\n",
    "dataframe_global_output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vH3S2ZAQyk4"
   },
   "outputs": [],
   "source": [
    "nb_by_batch_id = dataframe_global_output.groupby('BATCH_ID').count()\n",
    "nb_by_batch_id.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F43t8i1T415o"
   },
   "source": [
    "### Joining Input and Output DataFrames for Displaying the Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPt8OQYlO-XM"
   },
   "outputs": [],
   "source": [
    "# Perform the join on the columns 'batch_id', 'iox', and 'ioy'\n",
    "data_final_A = data_input_A.join(dataframe_global_output, on=['BATCH_ID', 'OUTPUT_MAPPING_X', 'OUTPUT_MAPPING_Y'], how='inner')\n",
    "\n",
    "# Show the result\n",
    "data_final_A.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yvnR6EYe4on"
   },
   "outputs": [],
   "source": [
    "nombre_de_ligne_A = data_final_A.count()\n",
    "\n",
    "print(\"Nombre distinct de batch id :\", nombre_de_ligne_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpAHg-Jq5CQe"
   },
   "source": [
    "### Grouping by \"OUTPUT\" with Statistics Calculation for Each Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EALFmwe8vtCU"
   },
   "outputs": [],
   "source": [
    "# Group by \"OUTPUT\", calculate statistics for each parameter\n",
    "sum_output_A = data_final_A.groupBy(\"OUTPUT\")\\\n",
    "    .agg(\n",
    "        avg(\"avg_param_1\").alias(\"avg_avg_param_1\"),\n",
    "        avg(\"q1_param_1\").alias(\"avg_q1_param_1\"),\n",
    "        avg(\"q2_param_1\").alias(\"avg_q2_param_1\"),\n",
    "        avg(\"q3_param_1\").alias(\"avg_q3_param_1\"),\n",
    "        avg(\"stddev_param_1\").alias(\"avg_stddev_param_1\"),\n",
    "        avg(\"avg_param_2\").alias(\"avg_avg_param_2\"),\n",
    "        avg(\"q1_param_2\").alias(\"avg_q1_param_2\"),\n",
    "        avg(\"q2_param_2\").alias(\"avg_q2_param_2\"),\n",
    "        avg(\"q3_param_2\").alias(\"avg_q3_param_2\"),\n",
    "        avg(\"stddev_param_2\").alias(\"avg_stddev_param_2\"),\n",
    "        avg(\"avg_param_3\").alias(\"avg_avg_param_3\"),\n",
    "        avg(\"q1_param_3\").alias(\"avg_q1_param_3\"),\n",
    "        avg(\"q2_param_3\").alias(\"avg_q2_param_3\"),\n",
    "        avg(\"q3_param_3\").alias(\"avg_q3_param_3\"),\n",
    "        avg(\"stddev_param_3\").alias(\"avg_stddev_param_3\"),\n",
    "        avg(\"avg_param_4\").alias(\"avg_avg_param_4\"),\n",
    "        avg(\"q1_param_4\").alias(\"avg_q1_param_4\"),\n",
    "        avg(\"q2_param_4\").alias(\"avg_q2_param_4\"),\n",
    "        avg(\"q3_param_4\").alias(\"avg_q3_param_4\"),\n",
    "        avg(\"stddev_param_4\").alias(\"avg_stddev_param_4\")\n",
    "    )\n",
    "\n",
    "# Show the result\n",
    "sum_output_A.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cntY0d7B5JwC"
   },
   "source": [
    "### Converting the Final DataFrame to a Pandas DataFrame for Creating an Excel File for Future Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVIXi1e5gLAh"
   },
   "outputs": [],
   "source": [
    "# Select the desired columns\n",
    "data_final_A_pd = data_final_A.toPandas()\n",
    "\n",
    "# Display the final result\n",
    "data_final_A_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXoYVDpx53a0"
   },
   "outputs": [],
   "source": [
    "# Specify the path where you want to save the Excel file\n",
    "excel_file_path = '/content/drive/MyDrive/output/data_final_A.xlsx'\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "data_final_A_pd.to_excel(excel_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
